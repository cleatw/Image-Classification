{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a note: I simply renamed the folder 'Capsicum' to 'Pepper' to better represent the vegetable to my audience. The rest of the original data is unaltered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of our classes of images\n",
    "classes = ['Bean', 'Bitter_Gourd', 'Bottle_Gourd', 'Brinjal', 'Broccoli', 'Cabbage', 'Carrot', 'Cauliflower', 'Cucumber', 'Papaya', 'Pepper', 'Potato', 'Pumpkin', 'Radish', 'Tomato']\n",
    "\n",
    "# create labels for them\n",
    "class_labels = {name:i for i, name in enumerate(classes)}\n",
    "\n",
    "num_classes = len(classes)\n",
    "\n",
    "image_size = (180, 180)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading our data\n",
    "\n",
    "def load():\n",
    "    directory = 'imgs'\n",
    "    categories = ['test', 'train', 'val']\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    for category in categories:\n",
    "        path = os.path.join(directory, category)\n",
    "        print(f\"Loading images at {path}...\")\n",
    "        \n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for file in os.listdir(path):\n",
    "            label = class_labels[file]\n",
    "            \n",
    "            #iterating through each image\n",
    "            for image in os.listdir(os.path.join(path, file)):\n",
    "                \n",
    "                #get the path name of each image\n",
    "                image_path = os.path.join(os.path.join(path, file), image)\n",
    "                \n",
    "                #open and then resize with our parameter set earlier\n",
    "                image = cv2.imread(image_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image, image_size)\n",
    "                \n",
    "                #append labels and images\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "                \n",
    "        images = np.array(images, dtype = 'float32')\n",
    "        labels = np.array(labels, dtype = 'int32')\n",
    "        \n",
    "        output.append((images, labels))\n",
    "   \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(test_images, test_labels), (train_images, train_labels), (val_images, val_labels) = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking our arrays for null values\n",
    "trainna = np.isnan(train_images).sum()\n",
    "testna = np.isnan(test_images).sum()\n",
    "valna = np.isnan(val_images).sum()\n",
    "\n",
    "print(trainna, testna, valna)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def show_examples(classes, images, labels):\n",
    "    \n",
    "    # shuffle our data to show random examples\n",
    "    images_shuff, labels_shuff = shuffle(images, labels, random_state=5)\n",
    "    \n",
    "    # show 5 images from our array with a label\n",
    "    for i in range(5):\n",
    "        plt.subplots()\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        image = cv2.resize(images_shuff[i], (image_size))\n",
    "        plt.imshow(image.astype(np.uint8))\n",
    "        plt.xlabel(classes[labels_shuff[i]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "show_examples(classes, train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a base model\n",
    "# these are just random layers I picked to try out for this test\n",
    "\n",
    "base_test_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (5,5), activation='relu', input_shape=(180, 180, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(5,5),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling our base model\n",
    "base_test_1.compile(optimizer='adam', \n",
    "                    loss='sparse_categorical_crossentropy', \n",
    "                    metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting and running our CNN\n",
    "base_test_1_fit = base_test_1.fit(train_images, \n",
    "                                  train_labels, \n",
    "                                  batch_size=50, \n",
    "                                  epochs=1, \n",
    "                                  validation_data=(val_images, val_labels), \n",
    "                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "around 39% accuracy for our base. not too great but it is just that - a base model. let's add more complexity to our CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying some new layers to try and improve model accuracy\n",
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (5,5), activation='relu', input_shape=(180, 180, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(3,3),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(3,3),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer='adam', \n",
    "                loss='sparse_categorical_crossentropy', \n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_1_fit = model_1.fit(train_images, \n",
    "                          train_labels, \n",
    "                          batch_size=50, \n",
    "                          epochs=3, \n",
    "                          validation_data=(val_images, val_labels), \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a solid improvement! let's try adding more complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding EVEN MORE LAYERS\n",
    "model_2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (2,2), activation='relu', input_shape=(180, 180, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer='adam', \n",
    "                loss='sparse_categorical_crossentropy', \n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_fit = model_2.fit(train_images, \n",
    "                          train_labels, \n",
    "                          batch_size=50, \n",
    "                          epochs=3, \n",
    "                          validation_data=(val_images, val_labels), \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you guessed it\n",
    "model_3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (2,2), activation='relu', input_shape=(180, 180, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(optimizer='adam', \n",
    "                loss='sparse_categorical_crossentropy', \n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_fit = model_3.fit(train_images, \n",
    "                          train_labels, \n",
    "                          batch_size=30, \n",
    "                          epochs=3, \n",
    "                          validation_data=(val_images, val_labels), \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, more complexity appears to help up until a certain point. Model 2 has the best performance, but let's try to tune it and see if we can push towards 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (2,2), activation='relu', input_shape=(180, 180, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_4_fit = model_4.fit(train_images, \n",
    "                          train_labels, \n",
    "                          batch_size=50, \n",
    "                          epochs=10, \n",
    "                          validation_data=(val_images, val_labels), \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98% accuracy for our model! wonderful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(model):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # plotting our accuracy\n",
    "    plt.subplot(221)\n",
    "    plt.plot(model.history['accuracy'],'bo--', label='accuracy')\n",
    "    plt.plot(model.history['val_accuracy'],'ro--', label='validation accuracy')\n",
    "    plt.title('training accuracy vs. validation accuracy')\n",
    "    plt.xlabel('# of epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plotting our loss\n",
    "    plt.subplot(222)\n",
    "    plt.plot(model.history['loss'],'bo--', label='loss')\n",
    "    plt.plot(model.history['val_loss'],'ro--', label='validation loss')\n",
    "    plt.title('training loss vs. validation loss')\n",
    "    plt.xlabel('# of epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss(model_4_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
